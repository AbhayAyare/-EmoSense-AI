# ğŸ¤– EmoSense-AI

**EmoSense-AI** is a hybrid real-time emotion detection system that combines facial expression analysis and voice tone recognition to predict human emotions. It uses deep learning, computer vision, and signal processing to power affective computing.

---

## ğŸ”¥ Features

- ğŸ¥ Real-time **facial emotion detection** using YOLOv8 and CNN
- ğŸ¤ Real-time **audio emotion recognition** using MFCC features and Conv1D models
- ğŸ”€ Option for **fusion model** combining both modalities
- ğŸ–¼ï¸ Streamlit-based interactive web interface
- ğŸ“¦ Modular project structure (easy to debug and extend)
- ğŸ’¾ Works with `.h5` models trained on FER2013 & audio emotion datasets

---

## ğŸ§  Model Architecture

### Facial Model
- Input: 48x48 grayscale face image
- CNN + Dense layers
- Trained on FER2013

### Audio Model
- Input: (174, 40) MFCC features from 2s audio clip
- Conv1D + Dropout + Dense
- Trained on preprocessed audio dataset

### Fusion Model 
- Input: Flattened features from both models
- Dense layers â†’ Output emotion

---

## ğŸ—‚ï¸ Project Structure

EmoSense-AI/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ templates/
â”‚ â”‚ â””â”€â”€ index.html # (optional if running Streamlit only)
â”‚ â””â”€â”€ streamlit_app.py # Streamlit real-time interface
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ audio/ # Raw audio files
â”‚ â”œâ”€â”€ video/FER2013/ # FER2013 facial images by emotion
â”‚ â””â”€â”€ processed/ # Preprocessed .npy feature files
â”œâ”€â”€ inference/
â”‚ â”œâ”€â”€ realtime_dual.py # Run facial+audio side-by-side (no fusion)
â”‚ â””â”€â”€ realtime_fused.py # Run fused model
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ facial_model.h5
â”‚ â”œâ”€â”€ audio_model.h5
â”‚ â””â”€â”€ fusion_model.h5
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ facial/
â”‚ â”‚ â””â”€â”€ preprocess.py
â”‚ â”œâ”€â”€ audio/
â”‚ â”‚ â””â”€â”€ extract_features.py
â”‚ â””â”€â”€ fusion/
â”‚ â”œâ”€â”€ model_fusion.py
â”‚ â””â”€â”€ fuse_utils.py
â”œâ”€â”€ train_fusion.py # Train fusion model
â”œâ”€â”€ check_labels.py # Dataset label sanity checker
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

Create and activate virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# or
source venv/bin/activate  # Linux/Mac
---
Install dependencies
pip install -r requirements.txt
---
Run the app (Streamlit interface)
streamlit run app/streamlit_app.py
ğŸ§ª Run Emotion Detection (CLI Scripts)
â¤ Run Realtime Dual Emotion Recognition (facial + audio separately)
python inference/realtime_dual.py
â¤ Run Realtime Fused Emotion Recognition (if fusion model is trained)
python inference/realtime_fused.py
---
ğŸ§° Datasets Used
FER2013 (Facial)
RAVDESS / Custom Audio Dataset

ğŸ“ˆ Sample Output
Facial Prediction	Audio Prediction	Final Output
ğŸ˜ Neutral	ğŸ˜  Angry	ğŸŸ¨ Dual Display
ğŸ˜¢ Sad	ğŸ˜¢ Sad	âœ… Match Detected

